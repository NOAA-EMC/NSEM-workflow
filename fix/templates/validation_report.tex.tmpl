\documentclass[12pt]{article}

\usepackage[fleqn]{amsmath}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{color}
\usepackage{hyperref}
\usepackage{subfig}

\setlength{\baselineskip}{16.0pt}
\setlength{\parskip}{3pt plus 2pt}
\setlength{\parindent}{20pt}
\setlength{\oddsidemargin}{0.3cm}
\setlength{\evensidemargin}{0.3cm}
\setlength{\marginparsep}{0.75cm}
\setlength{\marginparwidth}{2.5cm}
\setlength{\marginparpush}{1.0cm}
\setlength{\textwidth}{150mm}

\newcommand{\pb}{\vfill \pagebreak}
\newcommand{\bpage}{\vfill \pagebreak \strut

\vspace{2.5in} \centerline{This page is intentionally left blank.}}
\newcommand{\bpagea}{\strut

\vspace{2.5in} \centerline{This page is intentionally left blank.}}

%%%%% The above material is the preamble

\begin{document}

\pagestyle{empty}

\begin{center} 
U. S. Department of Commerce \\
National Oceanic and Atmospheric Administration \\
National Weather Service \\
National Centers for Environmental Prediction \\
5830 University Research Court \\
College Park, MD 20740

\vspace{15mm}

{\bf Technical Note}

\vspace{15mm}

{\large COASTAL Act Named Storm Event Model Validation Report for Hurricane \MakeUppercase{$storm}} \\

\vspace{15mm}

The Named Storm Event Model (NSEM) Development Group \\
\strut \\
NWS/NCEP/Environmental Modeling Center \\
NOS/OCS/Coast Survey Development Lab \\
NWS/OWP/National Water Center

\vfill

$date \\
(DRAFT) \\
\vspace{\baselineskip}
\end{center}
 
\vfill

\noindent \rule{140mm}{0.5mm} \\
{\small $^\dag$ MDAB Contribution No.~XXX.}

\bpage

\pb

\markboth{COASTAL Act}
         {{\rm DRAFT} \hspace{20.5mm} COASTAL Act NSEM Validation Report \MakeUppercase{$storm}}
\pagestyle{myheadings}
\pagenumbering{roman}
\setcounter{page}{1}

%\cftsetpnumwidth{3em}
%\cftsetrmarg{4em}  %% make sure that this is larger than above

\tableofcontents

\pb
\pagestyle{empty}
\bpagea

\pb
\pagestyle{myheadings}
\pagenumbering{arabic}

\section{Introduction}
\subsection{Scope}
The Consumer Option for an Alternative System To Allocate Losses (COASTAL) 
Act, which was included in the Biggert-Waters Flood Insurance Reform and 
Modernization Act of 2012, was enacted to help the Federal Emergency 
Management Agency (FEMA) determine the extent to which wind vs. water was 
the cause of damage in cases where little tangible evidence exists beyond 
a building's foundation following a tropical cyclone (also known as
`indeterminate losses' or `slabs'). This determination is needed for the 
proper and timely adjustment of insurance claims, as water damage is covered 
by FEMA's National Flood Insurance Program, while wind damage is covered by 
private insurers. The COASTAL Act requires the National Oceanic and Atmospheric 
Administration (NOAA) to produce detailed post-storm assessments following 
certain tropical cyclones impacting the U.S. and its territories. The assessments 
will be produced using a Named Storm Event Model (NSEM), which will indicate 
the strength and timing of damaging winds and water at a given location within 
the impacted area. NOAA is required to make post-storm assessment results and 
observations from the storm available to the public via the Coastal Wind and 
Water Event Database (CWWED). The NSEM is a coupled coastal wave and inundation 
model comprising of the components ADCIRC, WAVEWATCH III and the National Water 
Model. The coupling is achieved using the NOAA Earth Modeling System (NEMS) 
framework.

\subsection{Assessment area}
FEMA specified the assessment area as the area between the shoreline and the 
limit of coastal flooding during a Named Storm. This area should include all 
spatial coverage of USGS/FEMA post-event surveys, except for data points that 
did not pass QC. 

\subsection{Model validation} 
The purpose of this automated report is to summarize the results of the 90\% validation 
test of the Named Storm Event Model (NSEM) assessment for the storm event 
\MakeUppercase{$storm}. Section \ref{method} presents the methodology followed 
in the model validation. Section \ref{summary_result} provides an overview of 
these results for each covered data variable, followed by details of the 
best and worst performing stations in Section \ref{detail_result}. Section 
\ref{discuss} provides a general discussion of the presented results.

\section{Methodology} \label{method}

The performance of the Named Storm Event Model (NSEM) is validated at locations 
where in-situ observations are available. Since FEMA's damage equation critically 
depends upon the timing of wind and water impacts on the assessed structure, 
it is required to compare the time series of the covered data from the NSEM 
with a time series of observations, as opposed to, for example, maxima (such 
as high-water marks). For a given model run of the NSEM, we therefore need a  
test to compare the modeled time series to the observed time series. The level 
of accuracy is set in the COASTAL Act as 90\%, which is interpreted as a level 
of error of 10\%. We consider this requirement to be stricter than achieving 
a mean bias of $<$10\%, since a significant number of individual model data points 
could still differ by more than 10\% from the corresponding observation. At 
the same time, it is considered unreasonably strict to require that every model 
data point has an error of less than 10\%, considering the natural variability 
in the observed phenomenon (e.g. wind U10 wave height Hs), and observational 
error. As a result, the accuracy assessment will focus on the mean difference 
between the modeled and observed time series, and test whether this mean 
difference is below 10\%.

Since the model and observation both describe the same process (e.g. wind 
speed or wave height), there is a dependence between the modeled and observed 
time series variables. In this setting, the paired t-test hypothesis test is 
appropriate. To test whether the mean difference between these two time 
series is less than 10\%, we set the following null hypothesis $H_0$ and 
alternative hypothesis $H_a$:

\begin{equation}
\begin{aligned}
H_0: \mu_d < 0.1\\
H_a: \mu_d > 0.1
\end{aligned}
\end{equation}

where the mean relative difference is defined as $d_i = (X_{i,\mathrm{mod}} - X_{i,\mathrm{obs}})/X_{i,\mathrm{obs}}$, 
and $X_i$ is the model variable at time $i$ being tested. Since the alternative hypothesis 
states that the relative difference is greater than 0.1, this constitutes 
an upper-tailed test. This test has the following assumptions (Ott and Longnecker, 2017):

\begin{itemize}
\item That the sampling distribution of $d_i$ is a normal distribution.
\item That the $d_i$ samples are independent.
\end{itemize}

The hypothesis test is conducted at the standard level of significance of 
$\alpha$ = 0.05. This means that the null hypothesis that the mean difference 
between two time series at a given station is less than 0.1 (or 10\%) 
should be rejected if the p-value of this statistical test is $<$ 0.05. 
In practical terms, this means that the probability of erroneously rejecting 
the null hypothesis (that 90\% accuracy is met), given that it is true, is
less than 5\%. In the following sections, this validation is conducted for 
all stations within the assessment area, and presented in terms of 
overview results, followed by more extensive detail at select stations.

\section{Summary results} \label{summary_result}

Figures~\ref{fig:hwrf_summary} and~\ref{fig:ww3_summary} show the summary 
results of HWRF and WW3 respectively. In each case, the bar charts of p-value 
results by station and the geographical distribution of the 90\% accuracy 
test results are shown.  

\begin{figure}[h!tp]
\begin{center}
\subfloat[Bar chart of p-values by station. Red line: p-val = 0.05]{%
  \includegraphics[width=0.65\columnwidth]{$fig_hwrf_summary}%
}

\subfloat[Spatial plot of test results. Blue: Pass; Red: Fail]{%
  \includegraphics[width=0.65\columnwidth]{$fig_hwrf_map}%
}

\caption{\label{fig:hwrf_summary}Summary of 90\% accuracy test results for HWRF. }
\end{center}
\end{figure}

\begin{figure}[h!tp]
\begin{center}
\subfloat[Bar chart of p-values by station. Red line: p-val = 0.05]{%
  \includegraphics[width=0.65\columnwidth]{$fig_ww3_summary}%
}

\subfloat[Spatial plot of test results. Blue: Pass; Red: Fail]{%
  \includegraphics[width=0.65\columnwidth]{$fig_ww3_map}%
}

\caption{\label{fig:ww3_summary}Summary of 90\% accuracy test results for WW3. }
\end{center}
\end{figure}

\clearpage

\section{Detailed results} \label{detail_result}

This section provides details of the stations that best meet the 90\% accuracy criterion, versus those 
that had the poorest performance. Figures~\ref{fig:hwrf_best} and~\ref{fig:hwrf_worst} show the best and 
worst results for HWRF, and Figures~\ref{fig:ww3_best} and~\ref{fig:ww3_worst} show the corresponding results for WW3. 
Provided details are the (a) time series comparison between model and observation (b) error historgram, and (c) scatterplot.

\begin{figure}[htp]
\begin{center}
\subfloat[Time series]{%
  \includegraphics[width=0.55\columnwidth]{$fig_u10_ts_best}%
}
\subfloat[Histograms]{%
  \includegraphics[width=0.55\columnwidth]{$fig_u10_hist_best}%
}

\subfloat[Scatterplots]{%
  \includegraphics[width=0.55\columnwidth]{$fig_u10_scatter_best}%
}

\caption{\label{fig:hwrf_best}Details of best-performing station in terms of U10 wind speed.}
\end{center}
\end{figure}

\begin{figure}[htp]
\begin{center}
\subfloat[Time series]{%
  \includegraphics[clip,width=0.55\columnwidth]{$fig_u10_ts_worst}%
}
\subfloat[Histograms]{%
  \includegraphics[clip,width=0.55\columnwidth]{$fig_u10_hist_worst}%
}

\subfloat[Scatterplots]{%
  \includegraphics[clip,width=0.55\columnwidth]{$fig_u10_scatter_worst}%
}

\caption{\label{fig:hwrf_worst}Details of worst-performing station in terms of U10 wind speed.}
\end{center}
\end{figure}

\begin{figure}[htp]
\begin{center}
\subfloat[Time series]{%
  \includegraphics[width=0.55\columnwidth]{$fig_hs_ts_best}%
}
\subfloat[Histograms]{%
  \includegraphics[width=0.55\columnwidth]{$fig_hs_hist_best}%
}

\subfloat[Scatterplots]{%
  \includegraphics[width=0.55\columnwidth]{$fig_hs_scatter_best}%
}

\caption{\label{fig:ww3_best}Details of best-performing station in terms of significant wave height Hs.}
\end{center}
\end{figure}

\begin{figure}[htp]
\begin{center}
\subfloat[Time series]{%
  \includegraphics[clip,width=0.55\columnwidth]{$fig_hs_ts_worst}%
}
\subfloat[Histograms]{%
  \includegraphics[clip,width=0.55\columnwidth]{$fig_hs_hist_worst}%
}

\subfloat[Scatterplots]{%
  \includegraphics[clip,width=0.55\columnwidth]{$fig_hs_scatter_worst}%
}

\caption{\label{fig:ww3_worst}Details of worst-performing station in terms of significant wave height Hs.}
\end{center}
\end{figure}

\clearpage

\section{Discussion} \label{discuss}

To complete.

\end{document}
